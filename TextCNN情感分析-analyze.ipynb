{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 安装依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -upgrade pip\n",
    "# ! pip install jieba\n",
    "# ! pip install pandas\n",
    "# ! pip install numpy\n",
    "# ! pip install tensorflow==2.1.0\n",
    "#! pip install scikit-learn\n",
    "# ! pip uninstall tensorflow\n",
    "# ! pip install tensorflow-gpu==2.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_curve, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "valid_data = pd.read_csv('data/dev.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('data/test.tsv', sep='\\t') \n",
    "x_train, y_train = train_data.text_a.values, train_data.label.values # 训练集\n",
    "x_valid, y_valid = valid_data.text_a.values, valid_data.label.values # 验证集\n",
    "x_test, y_test = test_data.text_a.values, test_data.label.values # 测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 构建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\SMARTM~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.653 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "cut_docs = train_data.text_a.apply(lambda x: jieba.cut(x)).values #在训练集分词作为词表\n",
    "for doc in cut_docs: #提出每个文本\n",
    "    for word in doc: #提出文本内每个词\n",
    "        if word.strip():\n",
    "            vocab.add(word.strip())\n",
    "\n",
    "# 将词表写入本地vocab.txt文件\n",
    "with open('data/vocab.txt', 'w') as file:\n",
    "    for word in  vocab:\n",
    "        file.write(word)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 定义配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    embedding_dim = 300 # 词向量维度\n",
    "    max_seq_len = 200 # 文章最大词数\n",
    "    vocab_file = 'data/vocab.txt' # 词汇表文件路径\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 定义预处理类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self, config):\n",
    "        self.config = config #config：词表文件路径，最大词数，词向量维度\n",
    "        # 初始化词和id的映射词典，预留0给padding字符，1给词表中未见过的词\n",
    "        token2idx = {\"[PAD]\": 0, \"[UNK]\": 1} # {word：id}\n",
    "        with open(config.vocab_file, 'r') as reader:\n",
    "            for index, line in enumerate(reader):\n",
    "                token = line.strip()\n",
    "                token2idx[token] = index+2  #一个词赋值一个num字典\n",
    "        self.token2idx = token2idx #词典映射\n",
    "        \n",
    "    def transform(self, text_list):\n",
    "        # 文本分词，并将词转换成相应的id, 最后不同长度的文本padding长统一长度，后面补0\n",
    "        idx_list = [[self.token2idx.get(word.strip(), self.token2idx['[UNK]']) for word in jieba.cut(text)] for text in text_list]\n",
    "        idx_padding = pad_sequences(idx_list, self.config.max_seq_len, padding='post') #后面padding\n",
    "        return idx_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 定义模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    def __init__(self, config,kernel_sizes=[2,3,5],pooling_type=\"global_max_pooling\",k=3):\n",
    "        #配置预处理器\n",
    "        self.config = config\n",
    "        self.preprocessor = Preprocessor(config)\n",
    "        self.class_name = {0: '负面', 1: '正面'}\n",
    "        self.kernel_sizes = kernel_sizes#卷积核大小\n",
    "        self.pooling_type = pooling_type\n",
    "        self.k=k\n",
    "\n",
    "    def build_model(self):\n",
    "        # 模型架构搭建\n",
    "        idx_input = tf.keras.layers.Input((self.config.max_seq_len,))\n",
    "        #input_embedding : 输入: 2D 张量，尺寸为 (batch_size, input_length) 的张量。\n",
    "        #self.preprocessor.token2idx：词表大小,表示嵌入层可以嵌入的词数\n",
    "        #输出：3D 张量，尺寸为 (batch_size, input_length, output_dim)\n",
    "        input_embedding = tf.keras.layers.Embedding(len(self.preprocessor.token2idx),\n",
    "                    self.config.embedding_dim, #输出维度\n",
    "                    input_length=self.config.max_seq_len, #输入维度\n",
    "                    mask_zero=True)(idx_input)  #忽略零值，避免影响嵌入向量语义\n",
    "        convs = []\n",
    "        if self.pooling_type == \"global_max_pooling\":\n",
    "            for kernel_size in self.kernel_sizes:\n",
    "                c = tf.keras.layers.Conv1D(128, kernel_size, activation='relu')(input_embedding)\n",
    "                c = tf.keras.layers.GlobalMaxPooling1D()(c)  #不同池化策略是否有用?\n",
    "                convs.append(c)\n",
    "        elif self.pooling_type==\"max_pooling\":\n",
    "            for kernel_size in self.kernel_sizes:\n",
    "                c = tf.keras.layers.Conv1D(128, kernel_size, activation='relu')(input_embedding)\n",
    "                c = tf.keras.layers.MaxPooling1D(pool_size=2)(c)  # 使用常规最大池化\n",
    "                c = tf.keras.layers.Flatten()(c)  # 扁平化处理，以适应后续的全连接层\n",
    "                convs.append(c)\n",
    "        elif self.pooling_type==\"k-max-pooling\":\n",
    "             for kernel_size in self.kernel_sizes:\n",
    "                c = tf.keras.layers.Conv1D(128, kernel_size, activation='relu')(input_embedding)\n",
    "                c = tf.keras.layers.Lambda(lambda x: tf.nn.top_k(tf.transpose(x, [0, 2, 1]), k=self.k, sorted=True)[0])(c)\n",
    "                c = tf.keras.layers.Flatten()(c)  # 扁平化处理，因为k-max返回三维输出\n",
    "                convs.append(c)\n",
    "        else:\n",
    "            raise \"pooling type error!\"\n",
    "        \n",
    "        fea_cnn = tf.keras.layers.Concatenate()(convs)\n",
    "        \n",
    "        fea_cnn_dropout = tf.keras.layers.Dropout(rate=0.4)(fea_cnn)\n",
    "        \n",
    "        fea_dense = tf.keras.layers.Dense(128, activation='relu')(fea_cnn_dropout)\n",
    "        output = tf.keras.layers.Dense(2, activation='softmax')(fea_dense)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=idx_input, outputs=[output])\n",
    "        model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "        model.summary()\n",
    "        self.model = model\n",
    "    \n",
    "\n",
    "\n",
    "    def fit(self, x_train, y_train, x_valid=None, y_valid=None, epochs=5, batch_size=128, callbacks=None, id=None,**kwargs):\n",
    "        # 训练\n",
    "        self.build_model()\n",
    "        x_train = self.preprocessor.transform(x_train)\n",
    "        valid_data = None #评估矩阵\n",
    "        if x_valid is not None and y_valid is not None:\n",
    "            x_valid = self.preprocessor.transform(x_valid)\n",
    "            valid_data = (x_valid, y_valid)\n",
    "\n",
    "        # 添加历史记录回调\n",
    "        if callbacks is None:\n",
    "            callbacks = []\n",
    "        history = self.model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            validation_data=valid_data,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "        # # 绘制训练和验证的准确率曲线\n",
    "        # plt.figure(figsize=(12, 5))\n",
    "        # plt.subplot(1, 2, 1)\n",
    "        # plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "        # if valid_data is not None:\n",
    "        #     plt.plot(history.history['val_accuracy'], label='Validation Acc')\n",
    "        # plt.title('Model Accuracy')\n",
    "        # plt.ylabel('Accuracy')\n",
    "        # plt.xlabel('Epoch')\n",
    "        # plt.legend()\n",
    "\n",
    "        # # 绘制训练和验证的损失曲线\n",
    "        # plt.subplot(1, 2, 2)\n",
    "        # plt.plot(history.history['loss'], label='Train Loss')\n",
    "        # if valid_data is not None:\n",
    "        #     plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        # plt.title('Model Loss')\n",
    "        # plt.ylabel('Loss')\n",
    "        # plt.xlabel('Epoch')\n",
    "        # plt.legend()\n",
    "        # plt.savefig(f'train_history{id}.png')\n",
    "        return history\n",
    "        \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        # 评估\n",
    "        x_test = self.preprocessor.transform(x_test)\n",
    "        y_pred_probs = self.model.predict(x_test)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=-1)\n",
    "        result = classification_report(y_test, y_pred, target_names=['负面', '正面'])\n",
    "        print(result)\n",
    "\n",
    "        # # 绘制混淆矩阵\n",
    "        # cm = confusion_matrix(y_test, y_pred)\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Possitive'], yticklabels=['Negative', 'Positive'])\n",
    "        # plt.xlabel('Predicted Label')\n",
    "        # plt.ylabel('True Label')\n",
    "        # plt.title('Confusion Matrix')\n",
    "        # plt.savefig('confusion_matrix.png')\n",
    "\n",
    "        #  # 计算精确率和召回率\n",
    "        # precision, recall, _ = precision_recall_curve(y_test, y_pred_probs[:, 1])\n",
    "        # # 计算PR曲线下的面积\n",
    "        # pr_auc = auc(recall, precision)\n",
    "\n",
    "        # # 绘制PR曲线\n",
    "        # plt.figure()\n",
    "        # plt.plot(recall, precision, label=f'PR Curve (area = {pr_auc:.2f})')\n",
    "        # plt.xlabel('Recall')\n",
    "        # plt.ylabel('Precision')\n",
    "        # plt.title('Precision-Recall Curve')\n",
    "        # plt.legend(loc=\"upper right\")\n",
    "        # plt.savefig(\"P-R.png\")\n",
    "\n",
    "        # # 计算ROC曲线和AUC\n",
    "        # fpr, tpr, _ = roc_curve(y_test, y_pred_probs[:, 1])\n",
    "        # roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # # 绘制ROC曲线\n",
    "        # plt.figure()\n",
    "        # plt.plot(fpr, tpr, label=f'ROC Curve (area = {roc_auc:.2f})')\n",
    "        # plt.plot([0, 1], [0, 1], 'r--')  # 添加随机性能参考线\n",
    "        # plt.xlabel('False Positive Rate')\n",
    "        # plt.ylabel('True Positive Rate')\n",
    "        # plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        # plt.legend(loc=\"lower right\")\n",
    "        # plt.savefig(\"ROC.png\")\n",
    "        return result\n",
    "        \n",
    "        \n",
    "    def single_predict(self, text):\n",
    "        # 预测\n",
    "        input_idx = self.preprocessor.transform([text])\n",
    "        predict_prob= self.model.predict(input_idx)[0]\n",
    "        print(predict_prob)\n",
    "        predict_label_id = np.argmax(predict_prob)\n",
    "        \n",
    "        predict_label_name = self.class_name[predict_label_id]\n",
    "        predict_label_prob = predict_prob[predict_label_id]\n",
    "        \n",
    "        return predict_label_name, predict_label_prob\n",
    "    def test_predict(self,text):\n",
    "        # 预测\n",
    "        input_idx = self.preprocessor.transform([text])\n",
    "        [predict_prob,feacnn]= self.model.predict(input_idx)\n",
    "        print(f\"feacnn.shape:{feacnn.shape}\")\n",
    "        predict_label_id = np.argmax(predict_prob)\n",
    "        \n",
    "        predict_label_name = self.class_name[predict_label_id]\n",
    "        predict_label_prob = predict_prob[predict_label_id]\n",
    "        \n",
    "        return feacnn\n",
    "    \n",
    "    def load_model(self, ckpt_file):\n",
    "        self.build_model()\n",
    "        self.model.load_weights(ckpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 分析k-max-pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义early stop早停回调函数\n",
    "patience = 6\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型类，启动训练\n",
    "#\"max_pooling\",\n",
    "pooling_types=[\"k-max-pooling\"]\n",
    "model=[]\n",
    "for pooling_type in pooling_types:\n",
    "    print(\"pooling type:\",pooling_type)\n",
    "    config = Config()\n",
    "    for k in [2,3,4,5]:\n",
    "        textcnn = TextCNN(config,pooling_type=pooling_type,k=k)\n",
    "        model.append(textcnn)\n",
    "        textcnn.fit(x_train, y_train, x_valid, y_valid, epochs=50, callbacks=[early_stop]) # 训练\n",
    "        textcnn.evaluate(x_test, y_test) # 测试集评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 错误样本提取与分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本: 这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.6363525390625\n",
      "-----------------\n",
      "文本: 书一到，即被朋友借走，我是早已看过了的，买它，是因为张爱玲。看过张子静的《我的姊姊张爱玲》，可以看出，张对自己的弟弟也是不甚热情，这是她的性格。但是有一次张子静去找她，那天张爱玲却是很开心的样子。后来张子静推算，彼时张爱玲正与胡兰成相恋。很多人不耻于胡兰成的用情不专，我当然也很反感，但我也想说，胡兰成是给过张爱的欢喜的。张是怎样聪明的女子，若不是爱，她也不会那么痴。属于他们的爱情，让他们自己品尝与承担。胡的文字，确实是不错的，有一种儒雅在里面。不知道是不是那份淡淡的气氛，曾经让张心动——“岁月静好，现世安稳”。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.6333171725273132\n",
      "-----------------\n",
      "文本: 没有许多网友评价热度高的问题，第一次要手动安装winxp，主板的blis-硬盘重新设置\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.7051104307174683\n",
      "-----------------\n",
      "文本: 请问：有些字打错了，我怎么样才可以回去编辑一下啊？\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.7619051933288574\n",
      "-----------------\n",
      "文本: 风扇确实够响的，尤其是到晚上周围安静下来。风扇频频开启，发热量有些惊人\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.6541339755058289\n",
      "-----------------\n",
      "文本: 3999的时候抢购的 运气真好 这个价格还有什么号说的 品牌和价格都很不错\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9968169331550598\n",
      "-----------------\n",
      "文本: 硬盘到手就发现一个坏块，因为是完美屏，没回京东换新，花了两天在本地换新硬盘，发票都不需要；电池衔接很松，可有1mm间隙；出厂时A、B面贴的保护膜太敷衍，太多气泡，虽然反正要撕掉，但说明厂家态度不严谨。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5109139084815979\n",
      "-----------------\n",
      "文本: 没发现什么优点，回来了开机什么都没有，有自己装的XP也没费什么劲，麦有些问题。对方听不清。调了好久才好了，整体还不错\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.746155858039856\n",
      "-----------------\n",
      "文本: 原本在网上订了两个套房，入住后，携程还给我打电话问是否只入住了一间，要扣我信用卡里的钱。真不知道酒店与携程是如何衔接的？ 宾馆反馈 2007年12月7日 ： 我们非常感谢您的留言，非常抱歉由于一些小误会给您带来的困惑。根据我们的担保订房要求，对于确认担保的房间未入住的会收取当天房费。当然，我们也会进一步加强与携程的沟通与协调，让您今后的入住感觉更加舒适与愉快。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.6708174347877502\n",
      "-----------------\n",
      "文本: 第一次装xp到一半蓝屏，不过进bios，在advanced一栏里把硬盘模式从achi改为ide即可。驱动网上都有，下载g430驱动就行了\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.6904326677322388\n",
      "-----------------\n",
      "文本: 光驱确实不怎么好，装系统的时候就怕它挂掉，还好顺利装上系统了，装好后立马做了个镜像，估计以后也不用光驱了\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5912517309188843\n",
      "-----------------\n",
      "文本: 住了几次，这次还特意带了朋友一起去入住，结果，酒店的早餐让我们太失望了，越来越差啊，九点去的时候吃的东西都没有什么了，以后我是不会再来了。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.9720014929771423\n",
      "-----------------\n",
      "文本: 酒店还可以，房内有电脑，数字电视。早餐一般啦。隔音效果比较差，下面是商业街，不太安静。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.8410781621932983\n",
      "-----------------\n",
      "文本: 外观很有特点，长城外观，显然白天为了省电，大堂不开灯。。。似乎感觉上入住的人数有点少，另外就是入住手续有点意思，开什么入住通知书等等。。。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.788806676864624\n",
      "-----------------\n",
      "文本: ： 配置不错，奔腾双核感觉也不差~价格合适 不足： 装系统比较麻烦~ 总结： 不错，对得起这个价钱\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9972787499427795\n",
      "-----------------\n",
      "文本: 这书也能推荐？重点几乎都在为自己说好话树立好形象以及就关于某场官司的自辩上了，有钱就是好，还能出本这样的书～\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.636361837387085\n",
      "-----------------\n",
      "文本: 不知道携程怎么跟酒店协商的，明明连住两晚248，酒店偏要收我们同事275！害的我打电弧给携程和酒店协调这个事，酒店说销售部没上班，晕，销售部没上班你就别开店了！！虽然后来在携程的努力下酒店做出了补偿性的方案，但毕竟这样的遭遇确实让人很不舒服。其实我们一般去重庆都是住这里，也没有特别不如意的地方。但是这样看上去酒店方小小的失误，确给消费者非常不舒服的感觉，我想这事对富丽来讲不是偶然，期望富丽持续改进。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.6908465027809143\n",
      "-----------------\n",
      "文本: 刚看到第一本的名字《我想去看海》就觉得很可爱，然后看内容觉得还蛮有想象力的，呵呵，不错，应该可以激发小朋友的想象力吧~~~~~~~~~~~~~\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.968742847442627\n",
      "-----------------\n",
      "文本: 我现在上高中，英语成绩还算不错，这本书说是大学一级词汇，但其中大部分都是高中课本上的词汇，（我是新课改这届的）如果真是什么大学生要买的话我觉得没必要，里面的词汇应该都知道，这本书应该更适合初中生吧。还有这本书没我想象中那么逗，都是冷笑话- -。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.673201858997345\n",
      "-----------------\n",
      "文本: 08年8月19日我和先生、儿子入住武夷茶苑大酒店，房间空调不足，我们开窗通风，（窗为平移窗，打开后无法上锁）窗外环境极差，到处是垃圾堆、乱石堆。20日上午7点我的挎包和先生的手表失窃，报警立案，为小偷爬窗入室偷窃。这使我全家遭受不小的经济和精神损失（今后再也不敢住酒店一楼，惨痛的教训！我还很后怕地想，幸亏钱包内有不少现金，如果贼偷不到钱，狗急跳墙，我全家人的安全...）。由此可见，一家酒店能让窃贼登堂入室，其保安工作质量是相当差的，客人的人身安全也时刻受到威胁。而且，当我们发现失窃报总台后，保安人员却迟迟不到现场，我们最后只能自己报警。这次经历让我对武夷山的民风产生了疑惑，并且对该酒店的服务无法满意，对中、高层某些管理人员的态度感到气愤（一再强调我们住客该对被偷自负一定责任）。这是我住过的最糟糕的一家酒店也是我经历过的最糟糕的一次旅游！\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.522233247756958\n",
      "-----------------\n",
      "文本: 学到后面，发现不是都和笔记一样，就前面几章一样\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.7448073625564575\n",
      "-----------------\n",
      "文本: 送的包比较普通,做工也不好,和商城上99块那款东芝笔记本包是一样的,希望能送好点的包\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.692920446395874\n",
      "-----------------\n",
      "文本: 集成显卡,玩使命召唤5等要求很高的游戏比较卡,没有高清输出,用料没有以前老款结实了\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9316885471343994\n",
      "-----------------\n",
      "文本: 就性价比不错。无Express插槽。键盘键程短手感偏硬。显卡比08年的台式机的显卡差太多了\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.7234768867492676\n",
      "-----------------\n",
      "文本: 又是为了吸烟区和非吸烟区！一开始总是说没有，最后还都能找到， 唉！ 真不明白。 宾馆反馈 2008年7月8日 ： 您的宝贵意见得到了我们酒店管理层的高度重视,我们已经加强了从预订到前台对客人的吸烟房和无烟房的需求的重视.但如果酒店房间特别紧张时,酒店也只有将吸烟房做无烟处理,敬请谅解!欢迎您下次继续入住我酒店!\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.6927722096443176\n",
      "-----------------\n",
      "文本: 渡假村周围景色不错,但较落乡.硬件太差,服务水准有待提高!查房时故意用陈旧的烟蒂烫痕讹人,虽未得逞,但给人留下的感觉不好,以后不去就是了.\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.8686746954917908\n",
      "-----------------\n",
      "文本: XP声卡驱动是大问题 4799买的，啥也没送，现在4599还送大礼包，感觉好亏\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5418307781219482\n",
      "-----------------\n",
      "文本: 每个小鸡的动作和神态都不一样，很可爱，喜欢画画的儿子非常喜欢书的插图。文字很幽默，我们一家三口同看，笑得合不拢嘴\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.8609602451324463\n",
      "-----------------\n",
      "文本: 这本书是我08年所读的书中，较有收获之一。书中的一些观点打破我一些惯有观念。作者将二十岁年龄段里出现的迷惑作了一定的个人观点解答。值得处于这个年龄段的女孩看看。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7045674324035645\n",
      "-----------------\n",
      "文本: 我们7人同行，订了2间标房，1间三人房，到酒店入住时告知三人房与两间标房分别在两幢楼的，三人房check in的时候又告知房间有问题，还要给了我们两间标房，房间内设施齐全，但是洗澡水放出来就象泥水，要放15分钟才干净，楼内服务台没有服务人员，总体感觉不是很满意。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.8732749819755554\n",
      "-----------------\n",
      "文本: 体积较大，虽然是最早上网本品牌但质感较惠普dell等品牌还有差距，鼠标按键和快捷键很硬很难用，基本可以放弃触摸屏。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9980573058128357\n",
      "-----------------\n",
      "文本: 地点不错,离开汽车站不太远,走路十五分钟,打的两分钟就可以. 订房时送的早餐质量强差人意,但附近有麦当勞和肯德鸡. 房间挺宽,干净,但电器的插头是圆孔,有点不方便.宽带的插头离开桌子一段距离,不太方便.大堂服务员态度可以接受,不太冷.泳池的水是绿色,不是蓝色,太差了,我不敢下水了.\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.7316745519638062\n",
      "-----------------\n",
      "文本: 酒店的大堂真的很旧了，房间还可以，早餐的西餐厅比以前的就餐环境好了，以前乱哄哄的，（以前早餐是凭2张餐券，很多人估计叫外面的人来吃所以乱乱的），现在凭房卡就好多了。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.5935518741607666\n",
      "-----------------\n",
      "文本: 嘻嘻，书还没到我就给写了评，不过我告诉大家个小秘密，我没看结局！\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.811208963394165\n",
      "-----------------\n",
      "文本: 刚刚从事工作1年多，看完这本书真切的感受到了工作中好多不明白的事情，也更加坚定了自己心中的信念，很不错的一本书！\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9075784683227539\n",
      "-----------------\n",
      "文本: 跟很多买家一样遇到了表面金属标牌划痕问题，但知道没法退货，去商场看了样机，基本每台都有划痕，所以不知道是RP问题，还是普遍现象。使用中有散热问题，但基本能接受。虽然有蓝牙图标，但其实并没有硬件支持。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7241087555885315\n",
      "-----------------\n",
      "文本: 定位是网本, Linux系统是绝对够用了. 特别是Moblin的这个发布版本, 简直就是为这种只用来上网冲浪量身订造的. 界面设计, 还有功能键都十分方便贴心.\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.7759368419647217\n",
      "-----------------\n",
      "文本: 装xp小郁闷了一天。装之前需要先快速格盘，然后观点sata就可以了。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7496776580810547\n",
      "-----------------\n",
      "文本: 我于6月1日再次入住,住的是1312房,首先价格由238元涨到278元,是整个乌鲁木齐酒店旺季都涨了,据说每年7月和8月乌鲁木齐酒店都还要涨,这也可以理解,毕竟就那么几个月可以赚钱,冬天都是零下20几度,人很少.但是我发现房间的迷你吧撤了,只有留了两瓶矿泉水.卫\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5439252853393555\n",
      "-----------------\n",
      "文本: 10月24日入住西楼,房间很宽敞,但设施旧了一些,它的房间有两道门,这点很少见,所以晚上很安静,走廊的声音会小很多. 当时相同的价格,如果去附近的步行街上找一找,会有惊喜. 还好,电视信号还有,那天嫦娥升空,真巧.\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5663745999336243\n",
      "-----------------\n",
      "文本: 经朋友介绍，买来这本书。题目相当好，很有感染力。满怀期待地拜度后，才发现：内容零散、空洞，我越看越感到乏味。借给过已当父母的朋友，他们看后也与我同感。反正书本上唯一一个让我们有共同认同之处就是：要以孩子的角度去看待、体谅孩子，尊重、体谅都是必要的——这个道理即使未成父母的人都应该知道的，我们只是想得到更多实际的处理技巧。很可惜，这本书没有给到我们这方面很好的帮助！\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.6434456706047058\n",
      "-----------------\n",
      "文本: 晕死，发表的评论竟然无法编辑？？？我想改几个错别字就不行吗？？？“于己，是一泄郁闷的痛苦”应该是“于己，是一泄郁闷的痛快”一字之差，意思大变！！！\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.9829275012016296\n",
      "-----------------\n",
      "文本: 拿回来装系统的时后才发现屏幕有个坏点 当时验机的时候我就怕有坏点 dos系统没法验证 回来才发现 看来买电脑 最好带个安装盘 不然真是闹心~~~~\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5951322317123413\n",
      "-----------------\n",
      "文本: 挺悲哀的,住的是双标间国庆是230,在杭州住了个锦华之旅已经很挫了,这位仁兄有过之而无不及,依然的一阵阵怪味,纱窗坏的,空调乱叫,唯一的好处就是代售附近的景点(如:浙西大峡谷,天滩,大明山等)门票八五折,本身在携程订的是两晚,连忙把第二晚的给退了,听当天把我们带到天滩的司机介绍住在大峡谷镇的一个旅店.\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5443654656410217\n",
      "-----------------\n",
      "文本: 我订的是商务大床房，给我的房间是8418房，床是挺大的，但是房间太小了，基本上是进屋就上床了，连椅子都放不开，但是有个看不到海景的大阳台，不知道是怎么设计的，估计这房应该是个单人房改的。如家的电视节目有马赛克，给前台打电话，他们说叫工程师来看看，结果过了几分钟来了个拿对讲机的大婶，告诉我们是信号的原因，所有房间的节目都这样，要到晚上才能好，我们想那就算了，可是过会和我们同行来的另一个房间的朋友说他们的电视信号一直很好，气死了，如家竟然这样骗顾客，给前台打电话，又改口说是部分房间的信号是这样。晚上8点多的时候信号好了，但是第二天依然还是有马赛克，到了晚上也没好。住了三晚，拖鞋一直没给换，都破了。第三天蚊香片没给换。房间内的饮水机有很浓的84消毒水味，根本没法喝，自己到家乐福买的桶装水。只有一个床头灯，而且接触不良，不打它不亮。建议以后入住这家的顾客不要住这个带阳台的房间，没有阳台的房间还是很大的。 补充点评 2008年6月23日 ： 忘了说了，地址位置还算不错，公交车挺多，出去玩的话先去看看公交车的路线吧，不过车开的也很慢，我们坐了一辆从石老人始发的车回如家，结果那车最高时速才40迈，一点不夸张。步行去八大关也很近，也可步行去二浴，就是它的大门不是对着香港西路，有时出租车会估计要几个圈子才开到（青岛的出租司机良心大大的坏了，宁可被全国人民骂也要坑那几块钱)\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7271789908409119\n",
      "-----------------\n",
      "文本: 不值房价，早餐太糟，也许我去的比较晚，九点左右，没东西吃\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.7172694802284241\n",
      "-----------------\n",
      "文本: 问题多.刚买回来光驱弹不出来,必须用针弄出来！无奈之下第二天就去了维修站拆机修了，但有事还是不灵，后来又发现待机无法唤醒！打了客服电话说要我刷主板！如拿他那修机器还拿不回来！明显主板有问题\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.9595266580581665\n",
      "-----------------\n",
      "文本: 我不太敢相信自己的眼睛，这叫四星级！？我将镜头从卫生间地面上的一根卷曲的毛发摇到斑驳的电视机前（注意是“摇”，不是“跟”哦！），妄图找到理由，未果！后来费了半天的劲打开了窗子，看到了貌似繁荣中央大街一角，我恍然大悟，原来这里的卖点不是硬件和软件，而是附件！“借鸡生蛋”的典故不住的从脑袋里冒出……我觉得这样的价格住华旗或者福顺天天更好。由于郁闷，住上一晚就走了，不知道早餐怎么样，其他方面也不太清楚，整体感觉暗暗的、旧旧的。推荐企图忆苦思甜、加强爱国主义教育的朋友来住！\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7723764777183533\n",
      "-----------------\n",
      "文本: 这套书，是听别人介绍的，回家与3岁的女儿一起分享了故事内容，女儿越听越爱，现在我们会用故事里的对话在家玩游戏，真的很不错\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7317884564399719\n",
      "-----------------\n",
      "文本: 我在京东网买的华硕笔记本，第二天就发现严重的问题，无法使用。后来在华硕维修站的检测后发现是主板坏了，但由于检测过程中拖的时间很长时间，以至于超过15天包换期。现在，我要求换新机或者退货。你们不能卖坏掉的残次品给客户吧？！希望能给我一个明确的答复，谢谢。手机：13341772746\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.9607071876525879\n",
      "-----------------\n",
      "文本: 但是，书比网上有删节，例如删掉网络版中轻寒轻轻吻了一下阿离等情节。不知编辑怎么想的，这有必要吗？\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.5860801935195923\n",
      "-----------------\n",
      "文本: 条件一般，就是地理位置不错，离市中心比较近，出行还比较方便。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9975428581237793\n",
      "-----------------\n",
      "文本: 刚看到第一本的名字《我想去看海》就觉得很可爱，然后看内容觉得还蛮有想象力的，呵呵，不错，应该可以激发小朋友的想象力吧~~~~~~~~~~~~~\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.968742847442627\n",
      "-----------------\n",
      "文本: Win7全兼容，只需手动安装acpi驱动。 ubuntu9.04全兼容，建议装eeebuntu3.0，可支持软开关蓝牙无线摄像头。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.9853240847587585\n",
      "-----------------\n",
      "文本: 自带驱动光盘，光盘上有VISTA和XP驱动，还是傻瓜式的自动安装。安装完成后，完全可以正常使用。 模具和F80一摸一样，没有区别。 散热尚可。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.7821215391159058\n",
      "-----------------\n",
      "文本: 这个屏真爽啊，还没发现有什么点。反光也不强。30万像素的摄像头还挺清楚的。电池能用3个小时。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.9544625282287598\n",
      "-----------------\n",
      "文本: 酒店硬件和位置都好,但服务令人生气.12日大地震这一天,我通过携程入住了这酒店.第二天,因为办事延误了,没有来得及退房.下午携程问是否要退?但因已经过了十二点,我们要求延住,结果被告知二间房房价涨40元和50元了,说是星期一是特价,星期二取消特价了.我说我们连同住了两天应该是同一价,但酒店后来说,如果当时通过携程预订,住两天也是两个价的.如果退房的话,要收半天的费,还说三星以上宾馆加收10%服务费.总之感觉象是被宰了,今后是不会再入住这宾馆了. 补充点评 2008年5月14日 ： 补；由于天下雨，我们要向酒店借伞，并出示住房证，告知我们是酒店客人。谁知被告知先登记，再每人押100元，缺乏人性化。同样的事发生在凤凰佳居酒店，只要出示房证登记就可以了。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.831152617931366\n",
      "-----------------\n",
      "文本: 儿子现在两周九个月了,平时就有玩?下蛋游戏?,这回拿到的书,不仅看到?下蛋游戏?,还有?钓鱼游戏?等等,每天一看到我就抓我说,妈妈,我们一起到外面钓鱼去吧~要不,?妈妈,我下了个很大的蛋哦,我有个大屁股,哈哈,给你吃,我狂晕~?不过从内心上非常感动,儿子,自己懂得自娱自乐了,有时还会带领他的小伙伴们做游戏,当然,他就是老大,嘻!\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9844974279403687\n",
      "-----------------\n",
      "文本: 很纳闷的是第六期我没有收到卡片，我还想集齐了24张的。。这点最不满意\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.8306036591529846\n",
      "-----------------\n",
      "文本: 我一直不明白为什么这本书会有这么毒人关注，还是要保持清醒的头脑，认真鉴别\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7829369306564331\n",
      "-----------------\n",
      "文本: 已经第2次入住啦，老板对酒店非常满意哦，不然也不会订了，西西\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.6398296356201172\n",
      "-----------------\n",
      "文本: 原先一直想买非漫画版的，总觉得漫画版适合学生看，但是又找不到纯文字的那种，所以将就买了二十几本先看看，虽然一开始有点不习惯漫画版的阅读方式，但看了几页后就习惯了，并且越来越觉得这种版本比纯文字的更好，有视觉效果，这不，马上又订购了余下的四十多本，不过还缺第一本的，不知什么时候会有货，想买齐，收藏，等女儿大一点，再给她看。总之，值得推荐！\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.5821511149406433\n",
      "-----------------\n",
      "文本: 可能是看惯了台式机的液晶屏，感觉屏幕有些泛白，怎么调都不合适，另外本本确实有些厚重。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5050097107887268\n",
      "-----------------\n",
      "文本: 早餐是中式，有些东西很奇怪。 住店停车还收费，不应该。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.8146641850471497\n",
      "-----------------\n",
      "文本: Win7全兼容，只需手动安装acpi驱动。 ubuntu9.04全兼容，建议装eeebuntu3.0，可支持软开关蓝牙无线摄像头。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.9853240847587585\n",
      "-----------------\n",
      "文本: IBM 以前的笔记本非常的好，但现在的外观改动太大，性能方面先不说。我觉得有一个我非常不能接受的是外盖变成了钢琴烤漆，一不小心就划一道，手一摸就是一个印子。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5818807482719421\n",
      "-----------------\n",
      "文本: 但就对于《百家讲坛》的定位于中学生来说，本书定位还是比效合适，中学生看看还行\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7796489000320435\n",
      "-----------------\n",
      "文本: 改系统费了不少周折，最后整体都完好！缺点吗，刚好在腕托部分发热较大，有点影响情绪！\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.6320631504058838\n",
      "-----------------\n",
      "文本: 因为看了《士兵突击》的连续剧，爱它，所以买它。以为可以让自己感动的影视，其书更值得自己收藏。但真让人失望！\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.521923303604126\n",
      "-----------------\n",
      "文本: 刚刚买回来的时候发现书上写着终结，还以为这本书是最后一本，点知不是的。幸好还买了第六本，不是的话都不知道结果呢。在这本书知道陵容的真面目，真是吓了一跳。她怎么可以这样对她真是待她的姐妹呢。她怎可以忘记当年他父亲被甄寰救的事呢？权利果然是每个人都想要的，特别是在后宫这种地方，没有皇上的恩赐，权利、地位就是最大的，也是这种东西才能保住自己。后宫真是恐怖。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.5809930562973022\n",
      "-----------------\n",
      "文本: 酒店位于昆明中心区,地理位置不错,可惜酒店服务有些差,第一天晚上可能入住的客人不多,空调根本没开,打了电话问,说是中央空调要晚上统一开,结果晚上也没开,就热了一晚上,第二天有开会的入住,晚上就有了空调,不得不说酒店经济帐作的好.房间的床太硬,睡的不好.酒店的早餐就如其他人评价一样,想法的难吃. 不过携程的预订价钱还不错.\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.7789165377616882\n",
      "-----------------\n",
      "文本: 有些厚重吧，这跟京东没关系，呵呵，毕竟只花3999元银子嘛，这个价格比市面要低！\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.8519200682640076\n",
      "-----------------\n",
      "文本: 该酒店达不到五星水准。价格高，环境差，服务一般\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5362023711204529\n",
      "-----------------\n",
      "文本: 上册真的有点气愤，新发表的一篇短篇是很欣赏的，但是后面竟然加了和以前买的短篇集的内容一样，作者在后面有说明，但是真的是令人有种乱花钱的感觉。但是就是不能分开买，要整套买，哎，无尽叹气。下册我是超级没意见啦，下册才是真正想要的，但是还是那句话，要整套买。哎。。。。。。。。。。。。。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.5519749522209167\n",
      "-----------------\n",
      "文本: 送过来八本书，其他都是好好的，都套着塑料封皮，就这一本《活着》，不但没有外套，而且皱皱巴巴的，被蹂躏的不成样子，太影响阅读心情了。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7977321147918701\n",
      "-----------------\n",
      "文本: 招待所而已吧，唯一可取之处就是交通还行，三环边上。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9725009202957153\n",
      "-----------------\n",
      "文本: 酒店性价比极底, 早餐品种单调,老外一般只吃早餐,可这次不得不吃午饭;入住豪华层可房间很小,转身都困难,两张床只距离15CM远;马桶竟然没水;门弄了很久才打开;总体感觉是园林虽小还算可以,房间装修粗糙,我以为海德的产品一定不错,因为我住过398元的义乌海德大酒店,而扬州这间这是失礼人了.还收600多一晚 宾馆反馈 2008年7月14日 ： 非常抱歉由于我们工作的不到之处给您带来的麻烦，我们已专门召开了部门协调会对您提出的问题进行调查。非常希望您能与我们营销部联系，以便奉上我们的礼品表达我们的歉意。衷心感谢您提出的宝贵意见。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7246249318122864\n",
      "-----------------\n",
      "文本: 由于在南京住的银河索菲特感觉不错，所以这次来成都退了喜来登到万达索菲特，接机的师傅和行李员都非常的不错，但接下来发生的事情让我觉得非常气愤，问前台要了两张房卡，一张是开不了门的，然后去大堂换了张并出去吃东西，可回来发现在前台换得房卡还是开不了门，在走廊里等了十分钟才由行李员拿了张新的房卡，进房间发现夜床没有开，然后打电话给房屋中心来开也床，最可气的是我的房间尽能听见电梯机器的声音，吵得睡不着，最后打电话给大副换房间，半夜三点钟拿着大包小包在换房间，这是由此以来第一次发生在五星级酒店的事情！！！ 宾馆反馈 2008年3月25日 ： 酒店回复：非常感谢您的光临和对我们索菲特品牌的关注。我们对您在饭店遇到的不快表示歉意，也向您对我们饭店服务和设施中提出的不足致以诚挚的感谢，我们将加强前台员工的培训和监督，使服务质量提升，把品牌做得更好。并衷心的希望您有机会再来饭店体验我们各方面的提高。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.6547354459762573\n",
      "-----------------\n",
      "文本: 喜欢AMD的芯片，性价比不错。这款散热不错喜欢AMD的芯片，性价比不错。这款散热不错\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9944984316825867\n",
      "-----------------\n",
      "文本: 职场氦气的秘密却始终不会过时,每一位职场人士都可以拿来为己所用支持！\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.7288974523544312\n",
      "-----------------\n",
      "文本: 可以坐地铁8号线，在曲阜路下车出地铁就到了。西藏北路路口出。到对面就能找到国庆路。不过这个酒店的毛巾，浴巾相当的旧，而且还是黄色的那种，我都没有用过。还好只是住了一天。比较起来对面的晋元大酒店在这方面做的就好多了，旧是旧了点但是毛巾比较好，起码可以用。不过这个地方相对来说很安静。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9307863712310791\n",
      "-----------------\n",
      "文本: 我很支持,在数数的过程中,不要讲其他的话.1\\2\\3数字间隔中间的沉默,给孩子的心理行为会有更大的压力.不过,有时候,如果到了2,孩子还没静下来,我会说:?已经过了2了.?执行不久我觉得得给他一个过程来适应,所以,相当于有时候是数到四个数的....但是,绝对不跟孩子讲价格.\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.7027589678764343\n",
      "-----------------\n",
      "文本: dvd刻录没多大必要，虽然dvd刻录很普及，但是笔记本刻碟还是比较鸡肋的不如兑成优惠\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.6406639814376831\n",
      "-----------------\n",
      "文本: 采用了传统的SATA笔记本硬盘，型号是以ST开头，应该是希捷的，因此重量就稍微增加了，背在背包里面重了点，不过还是在情理之内的。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5444186925888062\n",
      "-----------------\n",
      "文本: 给孩子朗读的好处大家都知道,关键是如何读,读什么.书中举的例子都是国外的,没有中国孩子的读书名单.\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7402228116989136\n",
      "-----------------\n",
      "文本: 1. 屏幕有2个坏点。..................T_T.................. 2. 笔记本中部温度高，变压器温度很高。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.8227424621582031\n",
      "-----------------\n",
      "文本: 酒店的感觉比较旧，应该是装修的时间比较长了吧\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.8645679354667664\n",
      "-----------------\n",
      "文本: 一起买了好几本书，其他都还好，唯独这本，质量差的可以，很盗版。。。。。。。。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.6020473837852478\n",
      "-----------------\n",
      "文本: 1.有急事出去，要们童叫出租车，他们就叫酒店里的黑车，价格是普通出租价的两倍。你提出不要酒店的黑车时，他们就告诉你外面拦不到出租车，我们自己走出去时，外面出租车随时可以拦到。住店期间不止一次发生。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.8193291425704956\n",
      "-----------------\n",
      "文本: 什么时候降价,新品是CQ40-514TX 4200/2G/250G/512独,价格比404还低!!!!\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9941956400871277\n",
      "-----------------\n",
      "文本: 屏幕上有一个坏点，夜间测试，硬盘滴答滴答的响。 后面的螺丝都有卸过的痕迹，让人有种不放心的感觉！ 京东多的东西都送了，一个鼠标也表现得那么吝啬。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9090180993080139\n",
      "-----------------\n",
      "文本: 给孩子朗读的好处大家都知道,关键是如何读,读什么.书中举的例子都是国外的,没有中国孩子的读书名单.\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7402228116989136\n",
      "-----------------\n",
      "文本: 住这个酒店实在是太享受了,不仅可以使用五彩缤纷的白毛巾,还可以免费听赏别人KTV包厢里的高音演奏.!! 以下是某某歌奏家的话,,,,,谢谢 谢谢大家 首先我要感谢我的父母 还要感谢背后支持我的朋友们 让我有了以噪音感化大家的机会 现在我再为大家献上一首!!希望大家夜不能眠!!!!!!!\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.6836856603622437\n",
      "-----------------\n",
      "文本: 第一次在当当上买书,选了很久,最终选了<神奇校车><长袜子皮皮><不一样的卡梅拉>和<法布尔昆虫记>作为女儿六一的礼物,其中这本<法布儿昆虫记>是女儿最喜欢的,每晚必读,她还不识字都是由我来读,基本两晚就读完一本,说实话书里的语言对于小朋友非常地好懂,轻松地就能知道了昆虫的知识,一次在公园里,女儿还给她爸上了一节红蚂蚁和黑蚂蚁的课呢,对昆虫感兴趣的小朋友一定不能错过!\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.7646846771240234\n",
      "-----------------\n",
      "文本: 唯一的遗憾,书有些质量问题,开头三四页正中有很多装订的粗针扎的孔.\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.9773335456848145\n",
      "-----------------\n",
      "文本: 在电视上看到于老师的讲课觉得不错买了这本书。但是我想说，这本书没什么收藏价值。能看电视还是看电视吧\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5859043598175049\n",
      "-----------------\n",
      "文本: 介绍说是很好，看了之后，名不副实，说教，空洞。没有实际的事例远远不如另外一本书《遇见未知的自己》好。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5814262628555298\n",
      "-----------------\n",
      "文本: 用起来还不错，本人还有几张2000-100 1000-50 东券要的加qq 673946022\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9864235520362854\n",
      "-----------------\n",
      "文本: 这个酒店基本就是低价海南旅游团住的那种招待所吧，周围极度荒凉。距离三塔很近，但是在后/侧墙外，距离大门口还有一段距离大约五、六百米。 房间不小但是还是用普通球锁＋自己拿钥匙，很久没见过号称四星级但是这样子的招待所了，出门一定要按下屋那的那边，否则你就会彻底大开门一整天了。 另外，号称四星，但是结帐时候竟然不能提供消费详单（水单），而且从老总到客务部经理到 前台服务员都坚决没听说过住宿酒店给开了发票还要提供水单的。 服务业提议拿别的酒店的水单来看看， 是否能帮忙打印一下，但是客务部经理说“我们有电脑也有打印机，但是绝对不能给你用来打印！”问道这是代表个人还是酒店时候，答案是代表酒店。——导致我随手摔上了电话——手柄裂开了！ 至于物品损坏的赔偿，也是随口报价，注：一个普通的市话话机“原价”200大元，而且另外出去买一个座位赔偿是不被允许的！更可笑的是你如果表示罚多了，马上以态度不好而数额加倍，并且“四百块钱少一分钱也别想离开”（一把手,老板的原话）。只好争取好态度，交钱也不要发票来争取到“原价”赔付。 早饭有时候是自助，虽然东西不怎么样，但是十来种东西呢，吃饱没问题，但是住客少了就成了份儿饭了（号称 桌餐），品种也只有两三样了。 承诺和古城内的班车也没准儿，虽然因为是淡季，按照要求原先电话提前一小时联系了， 但是还是在原地打了4次催促的电话，等了30分钟车才到（实际酒店到古城距离2km左右）。 除了旅行社的低价团，建议大家就还是在古城内众多的酒店住下吧，价格一般是这里的一半，还可以方便地夜游古城。 对了， 你们谁见过房间不能上网的四星级标间吗？古城里面遍地是WLAN信号，我上网都是背着电脑去古城的。 在携程网上投诉后， 携程表示考虑下年是否还与这样的酒店签约了，还奖励500分。可惜时间、现金、心情的损失无法弥补。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.6515177488327026\n",
      "-----------------\n",
      "文本: 没买的就不用买了，到新浪网上读书频道看一眼足够了。毕大夫写作手法已经太老了，岁数也大的不适合写类似的东西了——居然还在为发现女同性恋大呼小叫么？这书里的故事哪有一件超出想象让人拍手称奇的呢？看不下去。。。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7041753530502319\n",
      "-----------------\n",
      "文本: 书的质量很好，纸张很厚实，画面很漂亮，图画很大，但都很幼稚，不适合三年级的孩子看，（我女儿就读三年级），我女儿说不喜欢，特别是一本英语单词的书，纯粹就是单词荟萃。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7978176474571228\n",
      "-----------------\n",
      "文本: 周边环境较差，服务的速度慢，态度还可以，价格太高。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5379011631011963\n",
      "-----------------\n",
      "文本: 这款笔记本的外观，使用的感觉都还不错，就是vista系统感觉反应有点慢，而且比较麻烦，使用程序时都提示是否继续，不知道能不能改这个设置，还没仔细看。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.6522307991981506\n",
      "-----------------\n",
      "文本: 完美屏 内存加到3个G 自己装的XP 真的很难找XP的恢复盘 IBM的启动项比较多 进程数有70个 Vista系统太慢了\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.7193010449409485\n",
      "-----------------\n",
      "文本: 除了环境比较好，在云龙公园对面，酒店设施太旧了，价格高，性价比低，在这个天气，房间里竟然有好多蚊子，半夜被蚊子打扰几次\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.8681278228759766\n",
      "-----------------\n",
      "文本: 优点： 外观够有型，配置很不错，价格合理，非常适合商务使用 不足： 光驱偶尔声音真的很大，底部发热量很大。 总结： 特价4999的价格购买还是很超值的，值得\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.9997816681861877\n",
      "-----------------\n",
      "文本: 4999买的,来咬我啊4999买的,来咬我啊4999买的,来咬我啊4999买的,来咬我啊 还送包和鼠标!!!哇哈哈哈哈~!!!\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.9397979974746704\n",
      "-----------------\n",
      "文本: 呵呵，又住了回豪华双床房，感觉房间比标间大了，没那么压抑，床也大了，比较舒服。就是电视太小了，农村的朋友家估计都没有这么小的电视了吧。卫生间还是太简陋了，和酒店的整体定位不匹配。衣柜里缺少浴袍，总可以提供两套吧，我住过的更便宜的酒店都有提供。\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.5572944283485413\n",
      "-----------------\n",
      "文本: 地段马马乎乎。但这个楼本来就像写字楼一样，大部分都是人家办公的地方，少部分楼层是客房。房间不大，设施也有点旧的样子，早餐更是残不目睹。我真的觉得他能评上3星是奇迹\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5238552093505859\n",
      "-----------------\n",
      "文本: 此书最受益是这两条1、心中无冷病，不怕热上身。2、肌肤不喜欢冷若冰霜的美人，美丽的女人一定要让身体暖和起来，其他所有的问题都会迎刃而解。说实话以前太不注意“寒”了，搞得自己现在身体很差，脾胃虚寒到了极点。开始认识到要对自己身体好并努力改善生活方式。现在已经好多了，多靠了这些仁心仁术的医生们的书。非常感谢！\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.6354501843452454\n",
      "-----------------\n",
      "文本: 书应该是因人而义吧，所以我觉得这本书没有多大的帮助．书的名字叫人蛮诱惑人的。但看了后感觉里面写的内容也很泛泛．没什么．买这本书时也是听朋友说了才买的，看了才觉得什么叫后悔．．．\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.5806933641433716\n",
      "-----------------\n",
      "文本: 整体尚可.优点很多就不说了.单就缺点罗列如下,望改进: 1,房内夜间有异响.象是水滴或是气流的声音. 2,宽带要服务台开通才可使用.开始不明白,耽误了好多事. 3,被子感觉很不好,不透气,象一层塑料罩着一样,很难受(个人感受). 4,落地灯插座没电.只好趴在桌下找桌后的插座. 5,卫生间隔音不是一般的差.\n",
      "真实标签: 正面, 预测标签: 负面, 预测概率: 0.8591886758804321\n",
      "-----------------\n",
      "文本: 没买的就不用买了，到新浪网上读书频道看一眼足够了。毕大夫写作手法已经太老了，岁数也大的不适合写类似的东西了——居然还在为发现女同性恋大呼小叫么？这书里的故事哪有一件超出想象让人拍手称奇的呢？看不下去。。。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.7041753530502319\n",
      "-----------------\n",
      "文本: 1.) 订了330的超豪华标房,房内有电脑,但每天上网收费50元.房内没有免费瓶装水. 2.) 第一次给了一个没有风景的房间,推开窗看到底楼餐厅的玻璃屋顶,强烈要求换房.然后换到一间,结果窗外两个大水箱,不知道是否CTRIP上订的房间都是这样的. 3.) 房间的空调有时会突然不制冷. 4.) 阳朔很美,还会再去,但肯定会换酒店.\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.995032548904419\n",
      "-----------------\n",
      "文本: 位置不很方便，周围乱哄哄的，卫生条件也不如其他如家的店。以后绝不会再住在这里。\n",
      "真实标签: 负面, 预测标签: 正面, 预测概率: 0.6051169633865356\n",
      "-----------------\n",
      "错误样本数: 115\n"
     ]
    }
   ],
   "source": [
    "#遍历测试集，找出错误的样本\n",
    "count=0\n",
    "#将错误结果整合到dict，以保存到csv中\n",
    "data=[]\n",
    "for i in range(len(x_test)):\n",
    "    text = x_test[i]\n",
    "    label = {0: '负面', 1: '正面'}[y_test[i]]\n",
    "    predict_label, predict_prob = textcnn.single_predict(text)\n",
    "    if label != predict_label:\n",
    "        count+=1\n",
    "        print(f'文本: {text}')\n",
    "        error_dict = {'text':[], 'true_label':[], 'predict_label':[], 'predict_prob':[]}\n",
    "        error_dict['text'].append(text)\n",
    "        error_dict['true_label'].append(label)\n",
    "        error_dict['predict_label'].append(predict_label)\n",
    "        data.append(error_dict)\n",
    "        print(f'真实标签: {label}, 预测标签: {predict_label}, 预测概率: {predict_prob}')\n",
    "        print('-----------------')\n",
    "print(f'错误样本数: {count}')\n",
    "error_df = pd.DataFrame(data)\n",
    "error_df.to_csv('error_samples.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

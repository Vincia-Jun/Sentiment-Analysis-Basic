{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 安装依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jun\\.conda\\envs\\Jun\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "valid_data = pd.read_csv('data/dev.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('data/test.tsv', sep='\\t') \n",
    "x_train, y_train = train_data.text_a.values, train_data.label.values # 训练集\n",
    "x_valid, y_valid = valid_data.text_a.values, valid_data.label.values # 验证集\n",
    "x_test, y_test = test_data.text_a.values, test_data.label.values # 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>房间太小。其他的都一般。。。。。。。。。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.接电源没有几分钟,电源适配器热的不行. 2.摄像头用不起来. 3.机盖的钢琴漆，手不能摸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>今天才知道这书还有第6卷,真有点郁闷:为什么同一套书有两种版本呢?当当网是不是该跟出版社商量...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9141</th>\n",
       "      <td>1</td>\n",
       "      <td>看过该书，感觉中医暂时不会消亡，尚有一、二十株老树活着，还有毛以林、黄煌、刘力红等一批有一定...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9142</th>\n",
       "      <td>0</td>\n",
       "      <td>这本书没读到底，不是特别喜欢。完全可以用序中的评价来表达我的感受：可以包容，却不想实践。除了...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9143</th>\n",
       "      <td>1</td>\n",
       "      <td>虽是观景房,不过我住的楼层太低(19楼)看不到江景,但地点很好,离轻轨临江门站和较场口站(起...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9144</th>\n",
       "      <td>1</td>\n",
       "      <td>性价比不错，交通方便。行政楼层感觉很好，只是早上8点楼上装修，好吵。 中餐厅档次太低，虽然便...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9145</th>\n",
       "      <td>0</td>\n",
       "      <td>跟心灵鸡汤没什么本质区别嘛，至少我不喜欢这样读经典，把经典都解读成这样有点去中国化的味道了</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9146 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                             text_a\n",
       "0         1  选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全...\n",
       "1         1  15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很...\n",
       "2         0                               房间太小。其他的都一般。。。。。。。。。\n",
       "3         0  1.接电源没有几分钟,电源适配器热的不行. 2.摄像头用不起来. 3.机盖的钢琴漆，手不能摸...\n",
       "4         1  今天才知道这书还有第6卷,真有点郁闷:为什么同一套书有两种版本呢?当当网是不是该跟出版社商量...\n",
       "...     ...                                                ...\n",
       "9141      1  看过该书，感觉中医暂时不会消亡，尚有一、二十株老树活着，还有毛以林、黄煌、刘力红等一批有一定...\n",
       "9142      0  这本书没读到底，不是特别喜欢。完全可以用序中的评价来表达我的感受：可以包容，却不想实践。除了...\n",
       "9143      1  虽是观景房,不过我住的楼层太低(19楼)看不到江景,但地点很好,离轻轨临江门站和较场口站(起...\n",
       "9144      1  性价比不错，交通方便。行政楼层感觉很好，只是早上8点楼上装修，好吵。 中餐厅档次太低，虽然便...\n",
       "9145      0      跟心灵鸡汤没什么本质区别嘛，至少我不喜欢这样读经典，把经典都解读成这样有点去中国化的味道了\n",
       "\n",
       "[9146 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       "        '15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错',\n",
       "        '房间太小。其他的都一般。。。。。。。。。', ...,\n",
       "        '虽是观景房,不过我住的楼层太低(19楼)看不到江景,但地点很好,离轻轨临江门站和较场口站(起点)很近,解放碑就在附近(大约100多公尺吧)!',\n",
       "        '性价比不错，交通方便。行政楼层感觉很好，只是早上8点楼上装修，好吵。 中餐厅档次太低，虽然便宜，但是和酒店档次不相配。',\n",
       "        '跟心灵鸡汤没什么本质区别嘛，至少我不喜欢这样读经典，把经典都解读成这样有点去中国化的味道了'], dtype=object),\n",
       " array([1, 1, 0, ..., 1, 1, 0], dtype=int64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 构建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Jun\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.673 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocal) = 35091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['广博', '香', '摩花', '鼓点', '谋求', '美美', '法子', '口语化', '写作者', '伪品']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set()\n",
    "cut_docs = train_data.text_a.apply(lambda x: jieba.cut(x)).values\n",
    "for doc in cut_docs:\n",
    "    for word in doc:\n",
    "        if word.strip():\n",
    "            vocab.add(word.strip())\n",
    "\n",
    "# 将词表写入本地vocab.txt文件\n",
    "with open('data/vocab.txt', 'w') as file:\n",
    "    for word in  vocab:\n",
    "        file.write(word)\n",
    "        file.write('\\n')\n",
    "        \n",
    "print(\"len(vocal) = %d\" % len(vocab))\n",
    "list(vocab)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 定义配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    embedding_dim = 300 # 词向量维度\n",
    "    max_seq_len = 200   # 文章最大词数 200\n",
    "    vocab_file = 'data/vocab.txt' # 词汇表文件路径\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 定义预处理类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # 初始化词和id的映射词典，预留0给padding字符，1给词表中未见过的词\n",
    "        token2idx = {\"[PAD]\": 0, \"[UNK]\": 1} # {word：id}\n",
    "        with open(config.vocab_file, 'r') as reader:\n",
    "            for index, line in enumerate(reader):\n",
    "                token = line.strip()\n",
    "                token2idx[token] = index+2\n",
    "                \n",
    "        self.token2idx = token2idx\n",
    "        \n",
    "    def transform(self, text_list):\n",
    "        # 文本分词，并将词转换成相应的id, 最后不同长度的文本padding长统一长度，后面补0\n",
    "        idx_list = [[self.token2idx.get(word.strip(), self.token2idx['[UNK]']) for word in jieba.cut(text)] for text in text_list]\n",
    "        tensor_list = [torch.tensor(sublist) for sublist in idx_list]\n",
    "        padded_sequences = [F.pad(sequence, (0, self.config.max_seq_len - sequence.size(0))) for sequence in tensor_list]\n",
    "        stacked_tensor = torch.stack(padded_sequences)\n",
    "        \n",
    "        return stacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35093"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor(config)\n",
    "res_show = preprocessor.transform(['性价比不错，交通方便。', '宝贝我爱你'])\n",
    "len(preprocessor.token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset\n",
    "x_train = preprocessor.transform(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "\n",
    "x_valid = preprocessor.transform(x_valid)\n",
    "y_valid = torch.LongTensor(y_valid)\n",
    "valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "\n",
    "x_test = preprocessor.transform(x_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataloader\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True,\n",
    "                            pin_memory = True,\n",
    "                            num_workers=num_workers)\n",
    "    \n",
    "valid_loader = DataLoader(valid_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True,\n",
    "                            pin_memory = True,\n",
    "                            num_workers=num_workers)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True,\n",
    "                            pin_memory = True,\n",
    "                            num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 定义模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, filter_sizes, num_classes, vocab_size, num_filters, emb_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv1d(emb_dim, num_filters, x) for x in filter_sizes]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def pool(self, out, conv):\n",
    "        out = self.relu(conv(out))\n",
    "        max_pool = nn.MaxPool1d(out.shape[-1])\n",
    "        out = max_pool(out)\n",
    "        out = out.squeeze(2)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        embedded = self.dropout(self.embedding(x))      # x = [batch_size, seq_len]\n",
    "        embedded = embedded.permute(0,2,1)              # embedded = [batch_size, seq_len, emb_dim]\n",
    "        output = [self.pool(embedded, conv) for conv in self.convs]\n",
    "        out = torch.cat(output, dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    # Train stage\n",
    "    def fit(self, data_loader, data_loader2, epochs):\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.004)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # for calculating loss and acc\n",
    "            train_loss = torch.zeros(1).to(device)\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            # for calculating accuracy\n",
    "            data_loader = tqdm(data_loader, ncols=100)\n",
    "            data_loader.set_description(f\"Epoch [{epoch}] [Train]\")\n",
    "\n",
    "            for step, data in enumerate(data_loader):\n",
    "\n",
    "                x_train, y_train = data\n",
    "                x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "                outputs = self(x_train)\n",
    "                _, predicted = torch.max(outputs, 1)  # 获取预测的类别\n",
    "                total_correct += (predicted == y_train).sum().item()  # 统计预测正确的数量\n",
    "                total_samples += y_train.size(0)  # 统计样本总数\n",
    "\n",
    "                loss = criterion(outputs, y_train)\n",
    "                train_loss = (train_loss * step + loss.detach()) / (step + 1)  # update mean losses\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                data_loader.set_postfix(Loss=train_loss.item(), Acc=total_correct/total_samples)\n",
    "\n",
    "                if not torch.isfinite(loss):\n",
    "                    print('WARNING: non-finite loss, ending training ', loss)\n",
    "                    sys.exit(1)\n",
    "\n",
    "            self.valid(data_loader2, epoch, criterion, device)\n",
    "\n",
    "\n",
    "    # Valid stage\n",
    "    def valid(self, data_loader, epoch, criterion, device):\n",
    "        \n",
    "        self.eval()\n",
    "        eval_loss = torch.zeros(1).to(device)\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # for calculating accuracy\n",
    "        data_loader = tqdm(data_loader,  ncols=100)\n",
    "        data_loader.set_description(f\"Epoch [{epoch}] [Valid]\")\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            for step, data in enumerate(data_loader):\n",
    "\n",
    "                x_valid, y_valid = data\n",
    "                x_valid, y_valid = x_valid.to(device), y_valid.to(device)\n",
    "                outputs = self(x_valid)\n",
    "                _, predicted = torch.max(outputs, 1)  # 获取预测的类别\n",
    "\n",
    "                total_correct += (predicted == y_valid).sum().item()  # 统计预测正确的数量\n",
    "                total_samples += y_valid.size(0)  # 统计样本总数\n",
    "\n",
    "                loss = criterion(outputs, y_valid)\n",
    "                eval_loss = (eval_loss * step + loss.detach()) / (step + 1)  # update mean losses\n",
    "                data_loader.set_postfix(Loss=eval_loss.item(), Acc=total_correct/total_samples)\n",
    "\n",
    "            self.save_model(epoch, round(total_correct/total_samples*100,2))\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.eval()\n",
    "        x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "        outputs = self(x_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        y_test, predicted = y_test.to('cpu'), predicted.to('cpu')\n",
    "        test_acc = accuracy_score(y_test, predicted)\n",
    "        test_f1  = f1_score(y_test, predicted)\n",
    "        print(f\"[Test ]: Accuracy: {test_acc}, F1-Score: {test_f1}\")\n",
    "\n",
    "    def save_model(self, epoch, val_acc):\n",
    "        ckpt_path = \"./checkpoints/TextCNN_epoch{}_valid{}.pth\".format(epoch, val_acc)\n",
    "        torch.save(self.state_dict(), ckpt_path)\n",
    "        print(\"Saving weight to [%s] successfully.\" % (ckpt_path))\n",
    "    \n",
    "    def load_model(self, ckpt_path):\n",
    "        self.load_state_dict(torch.load(ckpt_path, map_location='cpu'))\n",
    "        print(\"Loading weight from [%s] successfully.\" % (ckpt_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0] [Train]: 100%|███████████████████| 143/143 [01:02<00:00,  2.30it/s, Acc=0.725, Loss=0.704]\n",
      "Epoch [0] [Valid]: 100%|██████████████████████| 19/19 [00:04<00:00,  4.27it/s, Acc=0.853, Loss=0.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weight to [./checkpoints/TextCNN_epoch0_valid85.33.pth] successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1] [Train]: 100%|███████████████████| 143/143 [00:56<00:00,  2.54it/s, Acc=0.863, Loss=0.346]\n",
      "Epoch [1] [Valid]: 100%|█████████████████████| 19/19 [00:04<00:00,  4.38it/s, Acc=0.882, Loss=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weight to [./checkpoints/TextCNN_epoch1_valid88.25.pth] successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2] [Train]: 100%|███████████████████| 143/143 [01:11<00:00,  1.99it/s, Acc=0.919, Loss=0.228]\n",
      "Epoch [2] [Valid]: 100%|█████████████████████| 19/19 [00:07<00:00,  2.66it/s, Acc=0.877, Loss=0.495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weight to [./checkpoints/TextCNN_epoch2_valid87.67.pth] successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3] [Train]: 100%|███████████████████| 143/143 [01:07<00:00,  2.12it/s, Acc=0.944, Loss=0.174]\n",
      "Epoch [3] [Valid]: 100%|█████████████████████| 19/19 [00:05<00:00,  3.22it/s, Acc=0.883, Loss=0.543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weight to [./checkpoints/TextCNN_epoch3_valid88.33.pth] successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4] [Train]: 100%|██████████████████| 143/143 [01:09<00:00,  2.05it/s, Acc=0.969, Loss=0.0991]\n",
      "Epoch [4] [Valid]: 100%|█████████████████████| 19/19 [00:06<00:00,  2.90it/s, Acc=0.868, Loss=0.622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weight to [./checkpoints/TextCNN_epoch4_valid86.75.pth] successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5] [Train]: 100%|██████████████████| 143/143 [01:11<00:00,  1.99it/s, Acc=0.975, Loss=0.0907]\n",
      "Epoch [5] [Valid]: 100%|█████████████████████| 19/19 [00:05<00:00,  3.34it/s, Acc=0.899, Loss=0.528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weight to [./checkpoints/TextCNN_epoch5_valid89.92.pth] successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6] [Train]: 100%|███████████████████| 143/143 [01:12<00:00,  1.98it/s, Acc=0.957, Loss=0.168]\n",
      "Epoch [6] [Valid]: 100%|███████████████████████| 19/19 [00:08<00:00,  2.32it/s, Acc=0.9, Loss=0.756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weight to [./checkpoints/TextCNN_epoch6_valid90.0.pth] successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7] [Train]: 100%|███████████████████| 143/143 [01:13<00:00,  1.94it/s, Acc=0.976, Loss=0.122]\n",
      "Epoch [7] [Valid]: 100%|█████████████████████| 19/19 [00:06<00:00,  2.86it/s, Acc=0.892, Loss=0.754]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weight to [./checkpoints/TextCNN_epoch7_valid89.17.pth] successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "textcnn = TextCNN(filter_sizes = [3,4,5],\n",
    "                  num_classes = 2,\n",
    "                  vocab_size = len(preprocessor.token2idx),\n",
    "                  num_filters = 128,\n",
    "                  emb_dim = config.embedding_dim,\n",
    "                  dropout = 0.4)\n",
    "\n",
    "textcnn.fit(train_loader,valid_loader, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 测试评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test ]: Accuracy: 0.8866666666666667, F1-Score: 0.8934169278996865\n"
     ]
    }
   ],
   "source": [
    "textcnn.evaluate(x_test, y_test) # 测试集评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 离线加载预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weight from [./checkpoints/TextCNN_epoch5_valid90.2.pth] successfully.\n",
      "[Test ]: Accuracy: 0.8875, F1-Score: 0.8899755501222494\n"
     ]
    }
   ],
   "source": [
    "ckpt = \"./checkpoints/TextCNN_epoch5_valid90.2.pth\"\n",
    "textcnn = TextCNN(filter_sizes = [3,4,5],\n",
    "                  num_classes = 2,\n",
    "                  vocab_size = len(preprocessor.token2idx),\n",
    "                  num_filters = 128,\n",
    "                  emb_dim = config.embedding_dim,\n",
    "                  dropout = 0.4)\n",
    "textcnn.load_model(ckpt)\n",
    "textcnn.evaluate(x_test, y_test) # 测试集评估"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
